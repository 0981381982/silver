<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>Silver XR Subgroup</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/yegor256/tacit@gh-pages/tacit-css-1.5.1.min.css"/>
  </head>
  <body>
    <section>

    <header>
      <h1>Silver XR Subgroup</h1>
      <h2>Home Page</h2>
      <nav>
        <ul>
          <li><a href="index.html">XR Home</a></li>
          <li><a href="captioning/index.html">Captioning Guideline Development</a>
          <li><a href="minutes.html">Meeting Minutes</a></li>
          <li><a href="xr-fpwd-info.html">FPWD Info</a></li>
        </ul>
      </nav>
    </header>
    <article>
        <ul>
            <li>Initial draft of guideline working has been created</li>
            <li>Outcomes have been discussed</li>
            <li>How to tabs are being left out for FPWD</li>
            <li>1 method is complete, work is underway on a second</li>
            <li>Test tab and Scoring tab have been discussed and editorial notes are used to highlight areas that need further work.</li>
        </ul>
    </article>
    <article>
        <p><strong>Guideline:</strong> Provide captions and associated meta-data for all  audio content within XR environments.</p>
        
        <mark> Note: WCAG 2.2 continues this by saying <em>except when the media is a media alternative for text and is clearly labeled as such.</em></mark>

        <h2>Outcome 1: Captions are used to understand speech</h2>
        <p>Translates speech and  non-speech audio into alternative formats (e.g. captions) so media can be understood when sound is unavailable or limited. User agents and APIs support the display and control of captions</p>

        <ul>
            <li>Method 1: <a href="https://docs.google.com/document/d/1lUoy00PrdsJjsqfqv8T4U1KokDM9LqbEe2GMWOwsEM4/edit#heading=h.lnxtengy1q8c">Provides text equivalents of speech and non-speech audio</a></li>
            <li>Method 2: <a href="https://docs.google.com/document/d/1CZ8tA6co1xwrSvktLcs81ysLD5bPB7cVeuKHB02xg7k/edit#heading=h.9x6p5wsesxfq">Caption Reflow: Customisable context sensitive reflow of captions, subtitles and text content in XR environments.</a></li>
        </ul>

        <h2>Outcome 2: Caption Meta-Data is used to convey further information</h2>
        <p>Conveys information about the sound in addition to the text of the sound (for example, sound source, duration, distance and direction) so users know the necessary information about the context of the sound in relation to the environment it is situated in.</p>

        <h2>Outcome 3: Captions format allows for alternative devices</h2>
        <p>Provides captions and caption meta-data in alternative formats (for example, second screen or braille display) to allow users the opportunity to move caption and meta-data to alternative displays. For example, this benefits users without sound and vision, users who need assistive technology to magnify portions of the view, or users who have limited reach.</p>

        <h2>Outcome 4: Caption visual display can be customised</h2>
        <p>Provides customization of caption style and position to support people with limited vision or color perception. Customization options can benefit all users.</p>

        <h2>Outcome 5: Caption temporal display can be customised</h2>
        <p>Provides customization of caption timing to support people with limited manipulation, strength, or cognition.</p>
     
    </article>
  </section>

  </body>
</html>
